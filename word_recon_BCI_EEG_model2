import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

data_path = 'C:/Users/irisz/Downloads/BCI_Project_Data-20240301T033659Z-001/BCI_Project_Data/'

train_data = np.load(data_path + 'train_data.npy')
test_data = np.load(data_path + 'test_data.npy')
train_label = np.load(data_path + 'train_label.npy')
test_label = np.load(data_path + 'test_label.npy')

#To convert the data into PyTorch tensors
x_train_tensor = torch.Tensor(train_data)
y_train_tensor = torch.LongTensor(train_label)
x_test_tensor = torch.Tensor(test_data)
y_test_tensor = torch.LongTensor(test_label)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") #Setting GPU on your computer

train_dataset = TensorDataset(x_train_tensor.to(device), y_train_tensor.to(device)) # input data to Tensor dataloader
train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True) #  Batch size refers to the number of data sample
test_dataset = TensorDataset(x_test_tensor.to(device), y_test_tensor.to(device))
test_loader = DataLoader(test_dataset, batch_size=64,  drop_last=True,shuffle=False)

class EEGCNNLSTMClassifier(nn.Module):
    def __init__(self, num_classes=5):
        super(EEGCNNLSTMClassifier, self).__init__()

        self.conv1 = nn.Conv2d(64, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1)) 
        
        self.kernel_size = (1, 64)
        # Assuming stride of 1 and dilation of 1 for simplicity in calculation
        self.stride = (1, 1)
        self.dilation = (1, 1)
        input_width = 795  # Assuming the width of your input data is 795
        
        # Calculate padding
        padding_height = 0  # No padding needed in this dimension for kernel_size (1, x)
        padding_width = ((input_width - 1) * self.stride[1] + self.dilation[1] * (self.kernel_size[1] - 1)) // 2

        self.conv1 = nn.Conv2d(1, 32, kernel_size=self.kernel_size, stride=self.stride, padding=(padding_height, padding_width))


    def forward(self, x):
        print(f"Input shape: {x.shape}")  # Should be [N, 64, H, W]
        x = self.conv1(x)
        print(f"After Conv1 shape: {x.shape}")
        # Assuming x is of shape (batch_size, 1, 64, 795) - batch, channels, height (electrodes), width (time)
        x = F.relu(self.conv1(x))
        x = x.permute(0, 3, 2, 1) # Reorder dimensions to (batch, seq_len, num_channels, features)
        x = torch.flatten(x, start_dim=2) # Flatten the last two dimensions for LSTM
        x, (h_n, c_n) = self.lstm(x)
        x = x.reshape(x.shape[0], -1) # Flatten output for the fully connected layer
        x = F.relu(self.fc1(self.dropout(x)))
        x = self.conv1(x)
        return F.log_softmax(x, dim=1)
    
num_classes = 5 # setting final output class
model = EEGCNNLSTMClassifier().to(device)
criterion = nn.NLLLoss() # Use NLLLoss function to optimize
optimizer = optim.Adam(model.parameters(), lr=0.001) # Setting parameters learning rate = 0.001

num_epochs = 20 # setting training epochs (Number of training iterations)
for epoch in range(num_epochs):
    model.train()
    for data, labels in train_loader:
        data, labels = data.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')

model.eval() # Evaluate your model
correct = 0
total = 0

with torch.no_grad():
    for data, labels in test_loader:
        outputs = model(data)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = correct / total
print(f'Test Accuracy: {accuracy * 100:.2f}%')